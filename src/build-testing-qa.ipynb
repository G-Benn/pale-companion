{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "- Document Store: FAISS for this purpose, with the first few chapters / arcs (through 5-10) for speed purposes\n",
    "- Preprocessor: We'll be using a `max_seq_length=400` for each of the embedding models, iterating on the split length and split_overlap\n",
    "- Retriever: Embedding Retriever so we can work on the SBERT models. This should be the state-of-the-art retriever at the moment and works reasonably well.\n",
    "- Embedding Models: Testing 3: The [SBERT Ms Marco Distilbert](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b), [SBERT Ms Marco BERT](https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5), and my fine-tuned distilbert (link tbd).\n",
    "- End result methodologies:\n",
    "    - [Extractive Search](https://docs.haystack.deepset.ai/docs/ready_made_pipelines#extractiveqapipeline) as a baseline to find relevant documents and contextual quotes.\n",
    "    - [Summarization](https://docs.haystack.deepset.ai/docs/ready_made_pipelines#extractiveqapipeline) as a less freeform version of the Generative search.\n",
    "    - [Generative Seq2Seq](https://docs.haystack.deepset.ai/docs/ready_made_pipelines#generativeqapipeline) model using `vblagoje/bart_lfqa`. I may also experiment with the ELI5 bart (or try my own).\n",
    "- Test run location: If I'm able to get a GPU in Colab I may run it there for speed, otherwise local machine should be fine.\n",
    "- Test run questions: will be present in the notebook and representative of the few arcs + some off-the-wall questions.\n",
    "- Time the execution time (in FAISS so not a straight translation to ElasticSearch) to understand the tradeoffs.\n",
    "- Learn a reasonable Top K for the Retriever - likely 5-15.\n",
    "- Output: Question-Answer pairs keyed on their metadata about the above results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSOR_SPLIT_BY = 'word' # word, sentence\n",
    "PREPROCESSOR_SPLIT_LENGTH = 400 # 400,100 / 8,2\n",
    "PREPROCESSOR_SPLIT_OVERLAP = 20 # 20 / 1\n",
    "\n",
    "# EMBEDDING_MODEL = \"sentence-transformers/msmarco-distilbert-base-tas-b\" # listed above \n",
    "# EMBEDDING_MODEL_SHORTNAME = \"msmarco_distilbert\" # msmarco_distilbert, msmarco_bert, finetuned\n",
    "# EMBEDDING_MODEL = \"sentence-transformers/msmarco-bert-base-dot-v5\"\n",
    "# EMBEDDING_MODEL_SHORTNAME = \"msmarco_bert\"\n",
    "EMBEDDING_MODEL = \"../twig_otherverse_parahumans_adapted\"\n",
    "EMBEDDING_MODEL_SHORTNAME = \"finetuned\"\n",
    "EMBEDDING_MAX_SEQ_LENGTH = 500 # 500\n",
    "\n",
    "# OUTPUT_TYPE = \"GENERATIVE_BART\" # SUMMARTIVE_X, EXTRACTIVE\n",
    "OUTPUT_TYPE = \"SUMMARATIVE_PEGASUS\"\n",
    "OUTPUT_NBEAMS = 8 # 3, 8\n",
    "OUTPUT_MAXLENGTH = 500 # 200, 500\n",
    "\n",
    "RETRIEVER_TOP_KS = [5,10,20,50]\n",
    "\n",
    "FINAL_TEST_CHAPTER = 6 # Exclusive last chapter to be tested\n",
    "\n",
    "SUMMARY_NAME = f\"{PREPROCESSOR_SPLIT_BY}({PREPROCESSOR_SPLIT_LENGTH},{PREPROCESSOR_SPLIT_OVERLAP})_{EMBEDDING_MODEL_SHORTNAME}_{OUTPUT_TYPE}({OUTPUT_NBEAMS},{OUTPUT_MAXLENGTH})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"Who is Avery Kelly?\",\n",
    "    \"Who is Zed?\",\n",
    "\n",
    "    \"What is Toadswallow?\",\n",
    "    \"What is Miss?\",\n",
    "    \"What is the Forest Ribbon Trail?\",\n",
    "    \"What animal is Snowdrop?\",\n",
    "    \"What happened at the Awakening Ritual?\",\n",
    "\n",
    "    \"When does the Hungry Choir contest start?\",\n",
    "    \"When does Alpeana Operate?\",\n",
    "\n",
    "    \"Where is Kennet?\",\n",
    "    \"Where is the Arena?\",\n",
    "\n",
    "    \"Why was Avery chosen for Awakening?\",\n",
    "    \"Why is Maricaca a suspect?\",\n",
    "\n",
    "    \"How does Verona's Sight describe objects?\",\n",
    "    \"How long has the Carmine been dead?\",\n",
    "    \"How old is Matthew?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import time\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack import Document\n",
    "from haystack.nodes import PreProcessor, EmbeddingRetriever, Seq2SeqGenerator, TransformersSummarizer, FARMReader\n",
    "from haystack.pipelines import GenerativeQAPipeline, ExtractiveQAPipeline, SearchSummarizationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f'generated_comparison_files/{SUMMARY_NAME}.pkl'):\n",
    "    raise ValueError(\"This combination of parameters already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 75 chapters up to Arc 6\n"
     ]
    }
   ],
   "source": [
    "with open('../data/chapter_fmt_list.pkl','rb') as f:\n",
    "    all_chapters = pickle.load(f)\n",
    "chapters = [i for i in all_chapters if int(i['meta']['arc_number']) < FINAL_TEST_CHAPTER]\n",
    "print(f\"Testing with {len(chapters)} chapters up to Arc {FINAL_TEST_CHAPTER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe0e233c69c46858064cf2917f9b587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/75 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be working with 1564 documents from 75 chapters\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    split_by=PREPROCESSOR_SPLIT_BY,\n",
    "    split_length=PREPROCESSOR_SPLIT_LENGTH,\n",
    "    split_overlap=PREPROCESSOR_SPLIT_OVERLAP,\n",
    "\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_respect_sentence_boundary= PREPROCESSOR_SPLIT_BY=='word',\n",
    "    progress_bar=True, \n",
    "    add_page_number=True\n",
    ")\n",
    "docs = preprocessor.process(chapters)\n",
    "print(f\"We will be working with {len(docs)} documents from {len(chapters)} chapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old document store...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac84f96b470944eb91c22aaa53e66d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/1564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Removing old document store...\")\n",
    "    os.remove(\"faiss_document_store.db\")\n",
    "except OSError:\n",
    "    print(\"Are you sure the document store db exists?\")\n",
    "document_store = FAISSDocumentStore(embedding_dim=768, faiss_index_factory_str=\"Flat\", similarity='cosine') # We want to stick with Cosine Similarity because it works best with the SBERT models we use\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ee7a5695944126b9660fc62692c79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/1564 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac7e526953e447f8edae389c6a13881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    model_format=\"sentence_transformers\",\n",
    "    max_seq_len=EMBEDDING_MAX_SEQ_LENGTH,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72a8aa9a86b4f12a72dde1d4a0105cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b787b02bba245a588aa458f67920bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef80dd2cd242038e02c874737c8557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f218ed7b72244fdb00051f551f15142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d3bcba47f4fc5941a1a68e3140372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41531056838451ca4c05f1bd209b9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63659a2a24496b8eb8e8edf4812325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if OUTPUT_TYPE == \"GENERATIVE_BART\":\n",
    "    print(\"Configuring this in the testing iteration...\")\n",
    "    # TODO: generate a dict of params: generators / pipelines based on the factors\n",
    "    generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\", num_beams=OUTPUT_NBEAMS, max_length=OUTPUT_MAXLENGTH)\n",
    "    pipe = GenerativeQAPipeline(generator, retriever) # We specify the params later\n",
    "elif OUTPUT_TYPE == \"SUMMARATIVE_PEGASUS\":\n",
    "    summarizer = TransformersSummarizer(model_name_or_path=\"pszemraj/led-large-book-summary\", max_length=OUTPUT_MAXLENGTH)\n",
    "    pipe = SearchSummarizationPipeline(summarizer=summarizer, retriever=retriever,generate_single_summary=True,return_in_answer_format=True)\n",
    "else:\n",
    "    raise ValueError(\"Not Configured yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 of 16\n",
      "  TopK 1 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaef29e5b0cb4aa5aa6cddefecea3b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 2 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853c1ed9049c464abb44a16c93029ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 3 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b058b4785f4f4b0398526407ea5f480c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 4 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185aae63f9e440bba5c5cfa9f6ae3c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.nodes.summarizer.transformers:One or more of your input document texts is longer than the specified maximum sequence length for this summarizer model. Generating summary from first 16384 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "Question 2 of 16\n",
      "  TopK 1 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcf2b06e42d4213a9c2c924ae426a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 2 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ca5fddd33b4de38e0d5df54881656c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 3 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11997472bd0c48cc92a1ba196c87cc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 4 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdd64b212404c6f8bd17cfe33d0b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "Question 3 of 16\n",
      "  TopK 1 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf2b0d03d1c46dba313894e35d26600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 2 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d26f5282d1f40748966ec06087f3525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 3 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ae81e8f6e347bebd479191108db1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 4 of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gibson/pale-companion/pale-companion/lib/python3.10/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf1a20ec7af4e0c8f8b17f1bf38e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "Question 4 of 16\n",
      "  TopK 1 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b99024d59ee44ae86caba6721f8a035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 2 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb7af91943b47cb9e7e2cf8b563d208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 3 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b4239b410d4c41a4cb1d1cca555af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "  TopK 4 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0117812ea894c149217b9858d9beb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA problem\n",
      "Question 5 of 16\n",
      "  TopK 1 of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7162ab02c344489f852227a694230f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing 1-question flow\n",
    "all_combos = []\n",
    "        \n",
    "# To make the param matching a little simpler I've looped instead of doing pipeline.batch\n",
    "for q_idx,q in enumerate(TEST_QUESTIONS):\n",
    "    print(f'Question {q_idx+1} of {len(TEST_QUESTIONS)}')\n",
    "    for k_idx,r_topk in enumerate(RETRIEVER_TOP_KS):\n",
    "        print(f'  TopK {k_idx+1} of {len(RETRIEVER_TOP_KS)}')\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = pipe.run(\n",
    "                query = q,\n",
    "                params = {\n",
    "                    \"Retriever\": {\"top_k\":r_topk},\n",
    "                    # \"Generator\": {\"max_length\": maxlength, \"num_beams\": nbeam}\n",
    "                }\n",
    "            )\n",
    "            if OUTPUT_TYPE == \"GENERATIVE_BART\":\n",
    "                result['answers'][0].answer\n",
    "            elif OUTPUT_TYPE == \"SUMMARATIVE_PEGASUS\":\n",
    "                answer = result['answers'][0]['answer']\n",
    "        except Exception:\n",
    "            print(\"CUDA problem\")\n",
    "            answer = \"N/A\"\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time_seconds = end_time - start_time\n",
    "\n",
    "        d = {\n",
    "            'question': q,\n",
    "            'exec_time_seconds': execution_time_seconds,\n",
    "            'answer': answer,\n",
    "            # Input all params\n",
    "            'retriever_topk': r_topk,\n",
    "            'PREPROCESSOR_SPLIT_BY' : PREPROCESSOR_SPLIT_BY,\n",
    "            'PREPROCESSOR_SPLIT_LENGTH' : PREPROCESSOR_SPLIT_LENGTH,\n",
    "            'PREPROCESSOR_SPLIT_OVERLAP' : PREPROCESSOR_SPLIT_OVERLAP,\n",
    "\n",
    "            'EMBEDDING_MODEL' : EMBEDDING_MODEL,\n",
    "            'EMBEDDING_MODEL_SHORTNAME' : EMBEDDING_MODEL_SHORTNAME,\n",
    "            'EMBEDDING_MAX_SEQ_LENGTH': EMBEDDING_MAX_SEQ_LENGTH,\n",
    "\n",
    "            'OUTPUT_TYPE': \"GENERATIVE_BART\",\n",
    "            'OUTPUT_NBEAMS':OUTPUT_NBEAMS,\n",
    "            'OUTPUT_MAXLENGTH':OUTPUT_MAXLENGTH\n",
    "        }\n",
    "        all_combos.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'generated_comparison_files/{SUMMARY_NAME}.pkl','wb') as f:\n",
    "    pickle.dump(all_combos,f)\n",
    "print(f\"File written to {SUMMARY_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pale-companion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b34b26ca0ede8a9f125e364d487712040961b4d4c8262639b9c1a29ec53671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
