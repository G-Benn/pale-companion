{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Qz_dTSGf9TLQoVuoqeFicAS1FohKSfma","timestamp":1681618577218}],"mount_file_id":"1Qz_dTSGf9TLQoVuoqeFicAS1FohKSfma","authorship_tag":"ABX9TyO5/4XdTbqp7gcB2PtMrqnF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# What does this do?\n","Given a database of documents (focused around Wildbow serials), attempts to answers questions.\n","\n","# How does it do this?\n","For more details (or to recreate the steps yourself) please check out the Github README!\n","1. Vectorizes all chapters of the serials using a GPL fine-tuned SentenceTransformers models. These chapters are transformed into Documents of varying number of words (results can vary based on the sizes used) and stored in a database.\n","2. Processes your question into a vector of a similar format to the above.\n","3. Finds the Document(s) most similar to the question and returns them. This will also filter the documents.\n","4. From those documents, [Extract](https://docs.haystack.deepset.ai/docs/retriever#embedding-retrieval-recommended) a number of answer(s) that answer those questions.\n","5. Output the chapters + content that was used to inform the answer (since it's likely to be unsatisfactory). Use this as a source for what chapters are likely to have your answers or to get some raw content to paste into something else (chatGPT, etc) or interpret on your own.\n","\n","# How do I use it?\n","0. If at all possible, go to `Change Runtime Type` in `Resources` and choose `GPU`. This should significantly speed up the queries. I have the best luck past 7pm ET for availability, but it's not strictly necessary.\n","1. Ensure that the database files (`wildbow_150.index`,`wildbow_150.json`,`wildbow_150_sqldb.db`, etc) are in the same location as `DB_FOLDER_LOCATION`. Colab can be finicky about how it attaches to GDrive so this may need some finangling. I've tried to leave 2 solutions in that each may work (you shouldn't need both).\n","2. There are a few variables defined in Cell X that can slightly change how everything behaves - feel free to experiment!\n","  - NUM_BEAMS: How many different [beams](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24) the Generator will produce for answer generation. Default is 8.\n","  - MAX_LENGTH: The maximum length of the sequence that will be generated. Keep in mind that a sequence will most likely naturally terminate before this is reached. Default is 500.\n","  - GENERATOR_TOPK: The number of answers that a Generator will return. Default is 3.\n","  - DATABASE_NAME: The name of the database that we'll search through to answer the questions. Options are `pale`, `otherverse` (comprising of Pale, Pact, Poke, and Pate), `parahumans` (comprising of Worm, Glow-worm, and Ward), `twig`, and `wildbow` (comprising of everything).\n","  - DOCUMENT_LENGTH: The size of the documents used in the database. This can cause the generator to return different results because of the information it ingests. `150`, `250`, `400`, `mixed` (storing all 3 types) are available here.\n","2. Run the cells through X - this will set everything up properly.\n","3. In Cell Y, specify the following variables:\n","  - QUESTION: Your question that you wish to ask. Default is <Question with acceptable answer>\n","  - RETRIEVER_TOPK: the number of documents that will be used to answer the question. Default is 10.\n","\n","# Limitations\n","1. There is no ability to filter on series or chapters here due to [Database limitations](https://docs.haystack.deepset.ai/docs/document_store#choosing-the-right-document-store). To get around this, I've included a few different databases (utilizing the same underlying embeddings). Based on the series you select it should pull the correct db if everything has been formatted correctly. There is no way to filter to chapters. These may be fixed in a future version, but it requires a different Database setup.\n","\n","# How accurate are the answers?\n","You're best off thinking of this like you're asking a mediocre fanfiction writer what the answer is - while they're enthusiastic about the series they probably only read part of it and don't exactly have a commitment to accuracy.\n","While the answers are grounded in truth, they're prone to a number of pitfalls:\n","  - Generative models are prone to favor likely combinations of words over rare combinations. This is especially true here as there is a specialized vocabulary for these series - the Generator doesn't know _exactly_ why \"Other\" is special or why \"Scion\" appears so often here and is less likely to select it as a high-probability. \n","  - It's only as good as the data it can build off of. Not only are these _stories_ where we're supposed to read between the lines and infer information, but those stories are told from biased character perspectives who receive half-truths from those around them. It's very gullible!\n","\n","\n","\n","# Will there be any changes to this in the future?:\n","Nothing in-progress at the moment, but there's a few areas of potential here:\n","  - Change of database to allow filtering on metadata (eg only Pale, only chapters < 10.1, only Avery PoV, etc)\n","  - Different types of answer generation like Extractive or Summaritive. These are relatively simple switches to make through this framework, it's just a matter of interest.\n","  - A Generator that can better mimic the style of Wildbow's answers. This would rely on a source-of-truth set of Document:Summary data that would be difficult to compile. However, it would both provide a more familiar output style and potentially could teach the model some of the vocabulary quirks that are present in the worlds (eg Other in the PactVerse != other in common usage).\n","  - A better way to capture rare words (like Primordial) in the answers - in NLP this is called [temperature](https://lukesalamone.github.io/posts/what-is-temperature/). While you typically don't want high amounts of creativity in your QA system, it can help make up for other shortcomings like the lack of trained vocabulary. This would require [MonkeyPatching](https://stackoverflow.com/questions/5626193/what-is-monkey-patching) or other general modifications of the [Haystack source code](https://github.com/deepset-ai/haystack/blob/main/haystack/nodes/answer_generator/transformers.py#L465) which could be a large endeavor. \n","  - A better pretraining on real questions and answers. The pretraining here is done via a technique called [Generative Pseudolabeling](https://www.pinecone.io/learn/gpl/), but it's no substitute for stronger training data for domain adaption.\n","  - The inclusion of various Word-of-God quotes or user-submitted answers to other queries (ie Reddit, Discord). The difficulty here is parsing the data formats and gathering it into formats that 1) better inform the Document embeddings how to store the information, 2) can be searched efficiently, and 3) are correct.\n","  - Updating Pale chapters available - currently the initial dataset only includes Pale chapters up to 23.1. This would be an incremental update.\n","  - Incorporating the possibility of a MultiModal Transformer for the Extra Materials. Currently any information from the Pale Extra Materials are included via transcripts manually pulled from the site(many thanks to those who transcribed this information) but the possibility of searching text and image embeddings simultaneously is a possibility.\n","  - Incorporate the ability to switch to a batch of questions depending on how many questions are asked. Currently this will only work on 1 question at a time."],"metadata":{"id":"4xkyYbD-Xajo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YzfjrlQXRaz"},"outputs":[],"source":["DATABASE_FOLDER_LOCATION = \"./drive/MyDrive/pale-companion-files/db-gen/\"\n","NUM_BEAMS = 32\n","MAX_LENGTH = 500\n","GENERATOR_TOPK = 3 \n","\n","DATABASE_NAME = \"pale\" # pale, otherverse, parahumans, twig, wildbow.\n","DOCUMENT_LENGTH = 'mixed' # 150, 250, 40, 'mixed'"]},{"cell_type":"code","source":["!pip install \"faiss-gpu>=1.6.3,<2\"\n","!pip install \"sqlalchemy <2\"\n","!pip install \"farm-haystack==1.14.0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACsd6Zlsm1IB","outputId":"5364903e-b8ea-4a4b-9247-2bbc314da26c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: faiss-gpu<2,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (1.7.2)\n","^C\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sqlalchemy<2 in /usr/local/lib/python3.9/dist-packages (1.4.47)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy<2) (2.0.2)\n"]}]},{"cell_type":"code","source":["import os\n","import pickle\n","import logging\n","import time\n","import textwrap\n","from haystack.document_stores import FAISSDocumentStore\n","from haystack import Document\n","from haystack.nodes import PreProcessor, EmbeddingRetriever, Seq2SeqGenerator, TransformersSummarizer, FARMReader\n","from haystack.pipelines import GenerativeQAPipeline, ExtractiveQAPipeline, SearchSummarizationPipeline"],"metadata":{"id":"9dTw116UnQLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n","logging.getLogger(\"haystack\").setLevel(logging.DEBUG)"],"metadata":{"id":"M5PjOt8RnXQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use this solution if you get a popup with no code when you attach GDrive\n","os.chdir(f'{DATABASE_FOLDER_LOCATION}/{DOCUMENT_LENGTH}') # Note - you'll need to reset the kernel or comment this line out if this has already been run\n","print(os.getcwd()) # This should match your GDrive/db/doclength folder\n","print([f for f in os.listdir('.') if os.path.isfile(f)])\n","# Use this solution if you have to run a code cell to attach GDrive\n","# TODO\n","document_store = FAISSDocumentStore.load(\n","    index_path=f'{DATABASE_NAME}_{DOCUMENT_LENGTH}.index' if not DOCUMENT_LENGTH=='mixed' else f'{DATABASE_NAME}.index',\n","    config_path=f'{DATABASE_NAME}_{DOCUMENT_LENGTH}.json' if not DOCUMENT_LENGTH=='mixed' else f'{DATABASE_NAME}.json'\n","    )\n","document_store.get_embedding_count() "],"metadata":{"id":"P7QhCpZgnq6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever = EmbeddingRetriever(\n","    document_store=document_store,\n","    embedding_model=\"TheSpaceManG/wildbow-distilbert\", # The name of model to create embeddings with. This will pull from HuggingFace and can be changed out as desired (eg distilbert-base-uncased-distilled-squad).\n","    model_format=\"sentence_transformers\",\n","    max_seq_len=500,\n","    progress_bar=True,\n",")"],"metadata":{"id":"EP0ss7YloAtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\", num_beams=NUM_BEAMS, max_length=MAX_LENGTH, top_k=GENERATOR_TOPK)\n","\n","pipe = GenerativeQAPipeline(generator, retriever) # We specify some of these params later"],"metadata":{"id":"ZgSyBAjBpBSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Given the format of the answer from our retrieval pipeline, parse out in a reasonable format\n","# 1. The answer(s) of which there will be K\n","# 2. The metadata about the answers, of which there will be k sets we want to break down into a few categories\n","# What series, what PoVs, what chapters, etc\n","def format_answers(output, chapter_info=False, text_output=False):\n","  for answerset in output['answers']:\n","    response = answerset.answer\n","    [print(x) for x in textwrap.wrap(f\"Answer: {response}\", width=120,break_long_words=False)]\n","    metadata = answerset.meta['doc_metas']\n","    if chapter_info:\n","      print('-----')\n","      print(f\"{len(metadata)} Documents\")\n","      for idx,x in enumerate(metadata):\n","        print(f\"Source {idx+1}: {x['arc_title']}, {x['chapter']}, {x['pov']}, {x['series']}\")\n","\n","    print('-----------------------------------------------------------')"],"metadata":{"id":"ZCp0l4Xexb7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QUESTION: Your question that you wish to ask! \n","QUESTION = \"Who is the Carmine?\"\n","\n","# The number of documents you want to parse to answer this.\n","# Personally I find that for the primitive Generative QA 15 is a decent number, but you should play with this\n","RETRIEVER_TOPK = 15\n"],"metadata":{"id":"U80X4pMQquBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will take 3-5 minutes or so, just be patient!\n","OUTPUT = pipe.run(\n","                query = QUESTION,\n","                params = {\n","                    \"Retriever\": {\"top_k\":RETRIEVER_TOPK}\n","                    }\n","            )"],"metadata":{"id":"67vG74BryH1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parse the output\n","format_answers(OUTPUT, chapter_info=True)"],"metadata":{"id":"4_FtTlDWyRVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VC8LVBdGM4TA"},"execution_count":null,"outputs":[]}]}